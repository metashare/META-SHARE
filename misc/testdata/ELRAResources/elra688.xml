<?xml version="1.0" encoding="UTF-8"?>
<resourceInfo xsi:schemaLocation="http://www.ilsp.gr/META-XMLSchema" xmlns:xslt="http://xml.apache.org/xsltm" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.ilsp.gr/META-XMLSchema">
<identificationInfo>
<resourceName lang="English">EvaSy Evaluation Package</resourceName>
<resourceName lang="French">Package d’évaluation EvaSy</resourceName>
<description lang="English">The EvaSy Evaluation Package was produced within the French national project EvaSy (Evaluation of speech synthesis systems), as part of the Technolangue programme funded by the French Ministry of Research and New Technologies (MRNT). The EvaSy project enabled to carry out a campaign for the evaluation of speech synthesis systems using French text data. This project is an extension of the only campaign that was ever carried out for French in this field within the AUPELF campaigns (Actions de recherche Concertées, 1996-1999). This package includes the material that was used for the EvaSy evaluation campaign. It includes resources, protocols, scoring tools, results of the campaign, etc., that were used or produced during the campaign. The aim of these evaluation packages is to enable external players to evaluate their own system and compare their results with those obtained during the campaign itself. The campaign is distributed over three actions: 1)	Evaluation of grapheme-to-phoneme conversion: it consists in evaluating the capacity of speech synthesis systems to phonetize text data.2)	Evaluation of prosody: it consists in evaluating the capacity of speech synthesis systems to forecast text prosody (duration and fundamental frequency of phonemes) from the text itself.3)	Global evaluation of the quality of speech synthesis systems:-	ACR tests (Absolute Category Rating): they consist in evaluating the overall quality of speech synthesis voices, by asking a number of subjects to evaluate several general characteristics of the speech synthesis voice, such as its naturalness, its fluency, its intelligibility.-	SUS tests (Semantically Unpredictable Sentences): they consist in evaluating the intelligibility of the speech synthesis voice, by using syntactically correct as well as semantically unpredictable sentences (which have no meaning).The EvaSy evaluation package contains the following data and tools:1) For the evaluation of the grapheme-to-phoneme conversion module: 1)	About 8,000 proper names (4,115 pairs firstname-surname) were extracted from Le Monde newspaper of 1992–2000 (over 200 million words), manually phonetised with variants and annotated with linguistic tags. The reference phonetisation was checked and corrected after the adjudication phase.2)	A corpus of emails (about 115,000 words) anonymised, segmented by paragraph and phonetised in SAMPA. The reference phonetisation was not checked. The evaluation of thos data was not carried out within EvaSy.3)	The SCLITE tool (developed by NIST) was used to compare the reference phonetisation with the one from the evaluated system, and to calculate the number of mistaken phonemes (inserted, forgotten or substituted phonemes).4)	The Post-align tool was used to align the reference phonetisation with the one from the evaluated system on a word-by-word basis. 2) For the evaluation of the prosodic module:-	Text data: 7 phonetically-balanced sentences extracted from the BREF corpus (cf. ELRA-S0067), with a duration lasting from 4 to 11 seconds.-	Speech data: 7 sentences read by one speaker.-	The Mbroli tool, which converts *.pho prosodic files into *.wav speech files, together with the MBROLA fr1 diphone database.-	The Mbrolign tool, which aligns the phonemes with the signal, extracts the prosodic parameters of the signal and copy them in the MBROLA diphone databas.3) For the global evaluation of the quality of speech synthesis systems:a)	For ACR tests (Absolute Category Rating):-	Text data: 40 abstracts with 5 sentences each of 20 second duration, extracted from the EUROM1f French corpus (cf. ELRA-S0014-01). -	Données audio : lecture des 40 passages par un locuteur EUROM1.b)	For SUS tests (Semantically Unpredictable Sentences):-	Text data: 24 lists of 12 SUS sentences. Phonemes are also distributed by list. -	Speech data: 24 lists read by a professional speaker.A description of the project is available at the following address:http://www.technolangue.net/article.php3?id_article=202 (in French language)</description>
<description lang="French">Le package d’évaluation EvaSy a été produit dans le cadre du projet national français EvaSy (« Evaluation des systèmes de Synthèse de parole »), issu du programme Technolangue, financé par le Ministère français délégué à la Recherche et aux Nouvelles Technologies (MRNT). Le projet EvaSy a permis de réaliser une campagne d'évaluation des synthétiseurs à partir du texte en français. Le projet se situe en partie dans la continuité de la seule campagne de ce type qui ait été conduite, dans le cadre des Actions de recherche Concertées de l'AUPELF (1996-1999). Ce package comprend l’ensemble des données utilisées lors de la campagne d’évaluation EvaSy. Il regroupe des ressources, des protocoles, des outils de notation, les résultats de la campagne officielle, etc., qui ont été utilisés ou produits pendant la campagne. Le but de ce « package » d’évaluation est de permettre à tout acteur externe de pouvoir évaluer son propre système et ainsi de pouvoir comparer ses résultats à ceux obtenus pendant la campagne.La campagne se décompose en trois actions : 1)	Evaluation de la conversion Graphème-Phonème : consiste à évaluer la capacité des systèmes de synthèse de la parole à phonétiser un texte.2)	Evaluation de la prosodie : consiste à évaluer la capacité des systèmes de synthèse de la parole à prédire la prosodie (durée et fréquence fondamentale des phonèmes) d’un texte à partir de ce texte.3)	Evaluation globale de la qualité des systèmes de synthèse :-	Tests ACR (Absolute Category Rating): consiste à évaluer la qualité globale de la voix de synthèse en demandant à des sujets d’évaluer plusieurs  caractéristiques génériques de la voix de synthèse comme son côté naturel, sa fluidité, son intelligibilité. -	Tests SUS (Semantically Unpredictable Sentences) : consiste à évaluer l’intelligibilité de la voix de synthèse en utilisant des phrases syntaxiquement correctes mais sémantiquement imprévisibles (qui n’ont pas de sens).Le package d’évaluation EvaSy contient les données et outils suivants :1) Pour l’évaluation du module de conversion Graphème-Phonème : -	Environ 8 000 noms propres (4 115 couples prénom nom) ont été extraits du journal Le Monde des années 1992–2000 (plus de 200 millions de mots), phonétisés manuellement avec variantes et annotés avec des étiquettes linguistiques. La phonétisation de référence a été vérifiée et corrigée après une phase d’adjudication.-	Un corpus d’emails (environ 115 000 mots) anonymisé, segmenté par paragraphe et phonétisé avec l’alphabet phonétique SAMPA. La phonétisation de référence n’a pas été vérifiée. L’évaluation de ces données n’a pas été retenue dans le cadre d’EvaSy.-	L’outil SCLITE (développé par le NIST) a été utilisé pour comparer la phonétisation de référence à celle du système évalué et de calculer le nombre de phonèmes en erreur (phonèmes insérés, oubliés, substitués).-	L’outil Post-align a été utilisé afin d’aligner mot à mot la phonétisation de référence à celle du système évalué. 2) Pour l’évaluation du module de prosodie :-	Données textuelles : 7 phrases phonétiquement équilibrées extraites du corpus BREF (cf. ELRA-S0067) et de durée variant de 4 à 11 secondes.-	Données audio : lecture des 7 phrases par un locuteur.-	L’outil Mbroli, qui transforme les fichiers prosodiques *.pho en fichiers audio *.wav, ainsi que la base de diphones MBROLA fr1.-	L’outil Mbrolign, qui aligne les phonèmes sur le signal, extrait les paramètres prosodiques du signal et les recopie sur la base de diphones MBROLA.3) Pour l’évaluation globale de la qualité des systèmes de synthèse :a)	Pour les tests ACR (Absolute Category Rating) :-	Données textuelles : 40 passages de 5 phrases chacun et d’une durée de 20 secondes extraits du corpus EUROM1f français (cf. ELRA-S0014-01). -	Données audio : lecture des 40 passages par un locuteur EUROM1.b)	Pour les tests SUS (Semantically Unpredictable Sentences) :-	Données textuelles : 24 listes de 12 phrases SUS. Les phonèmes sont également répartis par liste. -	Données audio : lecture des 24 listes par un locuteur professionnel.Une description du projet est disponible à l'adresse suivante :http://www.technolangue.net/article.php3?id_article=202</description>
<resourceShortName/>
<metaShareId>NOT_DEFINED_FOR_V2</metaShareId>
<url>http://catalog.elra.info/product_info.php?products_id=997</url>
<identifier>ELRA-E0023</identifier>
</identificationInfo>
<metadataInfo>
<metadataCreationDate>2005-05-12</metadataCreationDate>
<metadataCreator>
<surname lang="en-us">Valérie</surname>
<givenName lang="en-us">Mapelli</givenName>
<communicationInfo>
<email>mapelli@elda.org</email>
<url>http://www.elda.org</url>
<address>55-57 rue Brillat-Savarin</address>
<zipcode>75013</zipcode>
<city>Paris</city>
<country>France</country>
<telephoneNumber>+1 43 13 33 33</telephoneNumber>
<faxNumber>+1 43 14 33 30</faxNumber>
</communicationInfo>
</metadataCreator>
</metadataInfo>
<versionInfo>
<version>1.0</version>
<lastDateUpdated>2007-06-28</lastDateUpdated>
</versionInfo>
<distributionInfo>
<availability>available-restrictedUse</availability>
<licenceInfo>
<licence>ELRA_EVALUATION</licence>
<restrictionsOfUse>evaluationUse</restrictionsOfUse>
<price>150.00</price>
<userNature>academic</userNature>
<membershipInfo>
<member>True</member>
<membershipInstitution>ELRA</membershipInstitution>
</membershipInfo>
</licenceInfo>
<licenceInfo>
<licence>ELRA_EVALUATION</licence>
<restrictionsOfUse>evaluationUse</restrictionsOfUse>
<price>500.00</price>
<userNature>commercial</userNature>
<membershipInfo>
<member>True</member>
<membershipInstitution>ELRA</membershipInstitution>
</membershipInfo>
</licenceInfo>
<licenceInfo>
<licence>ELRA_EVALUATION</licence>
<restrictionsOfUse>evaluationUse</restrictionsOfUse>
<price>300.00</price>
<userNature>academic</userNature>
<membershipInfo>
<member>False</member>
<membershipInstitution>ELRA</membershipInstitution>
</membershipInfo>
</licenceInfo>
<licenceInfo>
<licence>ELRA_EVALUATION</licence>
<restrictionsOfUse>evaluationUse</restrictionsOfUse>
<price>1000.00</price>
<userNature>commercial</userNature>
<membershipInfo>
<member>False</member>
<membershipInstitution>ELRA</membershipInstitution>
</membershipInfo>
</licenceInfo>
<availabilityStartDate>2007-06-28</availabilityStartDate>
</distributionInfo>
<resourceCreationInfo>
<fundingProject>
<projectName>EVALDA</projectName>
<fundingType>nationalFunds</fundingType>
</fundingProject>
</resourceCreationInfo>
<resourceComponentType>
<corpusInfo>
<resourceType>corpus</resourceType>
<corpusMediaType>
<corpusTextInfo>
<mediaType>text</mediaType>
<lingualityInfo>
<lingualityType>monolingual</lingualityType>
</lingualityInfo>
<languageInfo>
<languageId>fre/fra</languageId>
<languageName>French</languageName>
</languageInfo>
<languageInfo>
<languageId>fre/fra</languageId>
<languageName>French</languageName>
</languageInfo>
</corpusTextInfo>
<corpusAudioInfo>
<mediaType>audio</mediaType>
<lingualityInfo>
<lingualityType>monolingual</lingualityType>
</lingualityInfo>
<languageInfo>
<languageId>fre/fra</languageId>
<languageName>French</languageName>
</languageInfo>
<languageInfo>
<languageId>fre/fra</languageId>
<languageName>French</languageName>
</languageInfo>
</corpusAudioInfo>
</corpusMediaType>
</corpusInfo>
</resourceComponentType>
</resourceInfo>
